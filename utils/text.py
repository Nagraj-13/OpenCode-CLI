import tiktoken

def get_tokenizer(model: str):
    try: 
        encoding = tiktoken.encoding_for_model(model)
        return encoding.encode
    
    except Exception:
        encoding = tiktoken.get_encoding("cl100k_base")
        return encoding.encode
    
def count_tokens(text: str, model:str)-> int:
    tokenizer = get_tokenizer(model)
    if tokenizer:
        return len(tokenizer(text))
    return estimate_tokens(text)

def estimate_tokens(text:str)-> int:
    return max(1, len(text) // 4)


def truncate_text(text:str, max_tokens:int, model:str, suffix: str = "\n... [Truncated]", preserv_line:bool=True):
    currnet_tokens = count_tokens(text)
    if currnet_tokens <= max_tokens:
        return text
    suffix_tokens = count_tokens(suffix, model)
    target_tokens = max_tokens - suffix_tokens
    
    if target_tokens <= 0:
        return suffix.strip()
    
    if preserv_line:
        return _truncate_by_lines(text, target_tokens, suffix, model)
    else:
        return _truncate_by_char(text, target_tokens, suffix, model)

def _truncate_by_lines(text:str, target_token:int, suffix:str, model:str):
    lines = text.spilt("\n")
    result_lines : list[str] = []
    current_tokens = 0
    for line in lines:
        line_token = count_tokens(line+ "\n", model)
        if current_tokens + line_token > target_token:
            break
        result_lines.append(line)
        
        current_tokens+=line_token
        
        if not result_lines:
            return _truncate_by_char(text,target_token,suffix, model)

def _truncate_by_char(text:str, target_token:int, suffix:str, model:str):
    low, high = 0, len(text)
    while low < high : 
        mid = (low + high+1)//2
        if count_tokens(text[:mid], model)<target_token:
            low=mid
        else:
            high=mid-1
    
    return text[:low]+suffix
